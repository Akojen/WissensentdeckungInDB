---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Übungsblatt 07"
author: "Alexander Kojen, Robin Czarnetzki, Jonas Kauke"  # Hier die Namen aller Gruppenmitglieder eintragen!
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
library("rpart")
        # die Pakete muessen gegebenenfalls installiert werden!
```
## Aufgabe 1
```{r}
load("lungcancer.RData")
```


### a)
Modell anpassen (Format der Daten beachten!)
```{r}
expVar <- cbind(lungcancer$FreqLunge,lungcancer$FreqSonst)
#For binomial and quasibinomial families the response can also be specified as a two-column matrix with the columns giving the numbers of successes (=FreqLunge) and failure

model <- glm(formula = expVar ~ Geschlecht + Alter + Raucherstatus + Ausbildung, family = "binomial", data = lungcancer)
summary(model)

```
Interpretation:
Alle Einflussgröße sind signifikant, jedoch kann der Einfluss des Alters nicht hinreichend interpretiert werden, da es sich dabei nicht um eine binäre Einflussgröße handelt.
Statistische Signifikanz ergibt sich aus den P-Werten (letzte Spalte der 'Coefficients' Tabelle), mit denen die Nullhypothese, dass die jeweiligen Einflussgrößen keinen Einfluss auf den Lungenkrebstod haben, mit einem Signifikanzniveau von a < 0.001 (0.1%) (***) abgelehnt werden kann.


### b)
Berechnung
```{r}
exp(summary(model)$coefficients)
#Geschlecht:    0.5
#Alter:         1.09
#Raucherstatus: 18.15
#Ausbildung:    0.75
```

Interpretation:
Männer starben doppelt so oft an Lungenkrebs wie Frauen.
Mit Zunahme Alters um ein Jahr, steigt die Wahrscheinlichkeit an Lungenkrebs zu sterben um ~9%.
Raucher starben ca. 18 mal so oft an Lungenkrebs wie Nichtraucher.
Die Wahrscheinlichkeit dafür, dass Teilnehmer mit Collegeausbildung an Lungenkrebs starben, ist ca. 25% geringer als die bei Teilnehmern ohne.


### c)
```{r}
#new.data[1,] <- 63j Raucher ohne C.Ausbildung / new.data[2,] <- 63j Nichtraucherin mit C.Ausbildung
new.data = data.frame(
  Geschlecht    = c(0,1),
  Alter         = c(63),
  Raucherstatus = c(1,0),
  Ausbildung    = c(0,1)
)
pred <- predict.glm(object = model, newdata = new.data, type = "response") #"type = "response" gives the predicted probabilities"
pred
pred[1] / pred[2] ##odds ratio für den Raucher ggü Nichtraucherin
```
Interpretation: Die Wahrscheinlichkeit dafür, dass 63jährige Raucher ohne C.Ausbildung an Lungenkrebs starben, war in etwa 44 Mal so hoch im Vergleich zur Wahrscheinlichkeit, dass 63jährigen Nichtraucherinnen mit C.Ausbildung an der selben Todesursache verstarben.


## Aufgabe 2

### a)
One-vs-All (One-vs-Rest):

Wenn in einem Trainingsdatensatz n Klassen vorhanden sind und die logistische Regression angewendet werden soll, so wird bei der One-vs-All-Methode für jede Klasse ein binärer Klassifikator bestimmt. 
Dabei ist die jeweilige Klasse als positiv und alle anderen zusammen als negativ definiert. 
Auf diesem Datensatz wird dann jeweils die logistische Regression angewandt, sodass wir für jede Klasse einen Klassifikator erhalten. 
Um nun einen neuen Datensatz x zu klassifizieren, wenden wir alle Klassifikatoren auf x an. 
Die Klasse mit dem maximalen Klassifikators für x wird dann gewählt.

### b)
One-vs-One:

Bei dieser Methode werden bei k Trainingsdatensätzen k*(k-1)/2 Klassifikatoren berechnet. 
So wird jeder Klassifikator durch das Unterscheiden von jeweils 2 Datensätzen trainiert. 
Soll nun ein neuer Datensatz x klassifiert werden, so werden wieder alle Klassifikatoren auf x angewandt. 
Jeder Klassifikator wählt eine der n Klassen für x aus. 
Diejenige Klasse, die am häufigsten gewählt wurde, wird von dieser Methode ausgewählt.


## Aufgabe 3
### a)

<!-- Es ist Ihnen freigestellt, ob Sie die Aufgabe im Markdown-Dokument lösen oder Sie die komplette Aufgabe 3 in einer separaten pdf-Datei abgeben -->

Für jeden Knoten des Baumes wird eine Tabelle der folgenden Form benötigt:

Entscheidung     |$S(t_L)$              |$S(t_R)$              |$i(T_L)$     | $i(T_R)$    | $p_L \cdot i(T_L) + p_R \cdot i(T_R)$
-----------------|----------------------|----------------------|-------------|-------------|---------------------------------------
$X_{1} \leq\ 1,5$|$1^2+0^2=1$           |$(4/9)^2+(5/9)^2=0,51$|$1-1=0$      |$1-0,51=0,49$|$1/10*0+9/10*0,49=0,44$
$X_{1} \leq\ 2,5$|$1^2+0^2=1$           |$(3/8)^2+(5/8)^2=0,53$|$1-1,00=0,00$|$1-0,53=0,47$|$2/10*0,00+8/10*0,47=0,37$
$X_{1} \leq\ 3,5$|$(2/3)^2+(1/3)^2=0,56$|$(3/7)^2+(4/7)^2=0,51$|$1-0,56=0,44$|$1-0,51=0,49$|$3/10*0,44+7/10*0,49=0,47$
$X_{1} \leq\ 4,5$|$(3/4)^2+(1/4)^2=0,62$|$(2/6)^2+(4/6)^2=0,55$|$1-0,62=0,38$|$1-0,00=0,45$|$4/10*0,38+6/10*0,45=0,42$
$X_{1} \leq\ 5,5$|$(4/5)^2+(1/5)^2=0,68$|$(1/5)^2+(4/5)^2=0,68$|$1-0,68=0,32$|$1-0,00=0,32$|$5/10*0,32+5/10*0,32=0,32$
$X_{1} \leq\ 6,5$|$(4/6)^2+(2/6)^2=0,55$|$(1/4)^2+(3/4)^2=0,62$|$1-0,55=0,45$|$1-0,00=0,38$|$6/10*0,45+4/10*0,38=0,42$
$X_{1} \leq\ 7,5$|$(5/7)^2+(2/7)^2=0,60$|$(0/3)^2+(3/3)^2=1,00$|$1-0,60=0,40$|$1-0,00=0,00$|$7/10*0,40+3/10*0,00=0,28$
$X_{1} \leq\ 8,5$|$(5/8)^2+(3/8)^2=0,53$|$(0/2)^2+(2/2)^2=1,00$|$1-0,53=0,47$|$1-0,00=0,00$|$8/10*0,47+2/10*0,00=0,37$
$X_{1} \leq\ 9,5$|$(5/9)^2+(4/9)^2=0,50$|$(0/1)^2+(1/1)^2=1,00$|$1-0,50=0,50$|$1-0,00=0,00$|$9/10*0,50+1/10*0,00=0,45$
                 |                      |                      |             |             |
$X_{2} \leq\ 1,5$|$1^2+0^2=1$           |$(4/9)^2+(5/9)^2=0,51$|$1-1=0$      |$1-0,51=0,49$|$1/10*0+9/10*0,49=0,44$
$X_{2} \leq\ 2,5$|$(1/2)^2+(1/2)^2=0,50$|$(4/8)^2+(4/8)^2=0,50$|$1-0,50=0,50$|$1-0,50=0,50$|$2/10*0,50+8/10*0,50=0,50$
$X_{2} \leq\ 3,5$|$(2/3)^2+(1/3)^2=0,56$|$(3/7)^2+(4/7)^2=0,51$|$1-0,56=0,44$|$1-0,51=0,49$|$3/10*0,44+7/10*0,49=0,47$
$X_{2} \leq\ 4,5$|$(3/4)^2+(1/4)^2=0,62$|$(2/6)^2+(4/6)^2=0,55$|$1-0,62=0,38$|$1-0,00=0,45$|$4/10*0,38+6/10*0,45=0,42$
$X_{2} \leq\ 5,5$|$(4/5)^2+(1/5)^2=0,68$|$(1/5)^2+(4/5)^2=0,68$|$1-0,68=0,32$|$1-0,00=0,32$|$5/10*0,32+5/10*0,32=0,32$
$X_{2} \leq\ 6,5$|$(4/6)^2+(2/6)^2=0,55$|$(1/4)^2+(3/4)^2=0,62$|$1-0,55=0,45$|$1-0,00=0,38$|$6/10*0,45+4/10*0,38=0,42$
$X_{2} \leq\ 7,5$|$(5/7)^2+(2/7)^2=0,60$|$(0/3)^2+(3/3)^2=1,00$|$1-0,60=0,40$|$1-0,00=0,00$|$7/10*0,40+3/10*0,00=0,28$
$X_{2} \leq\ 8,5$|$(5/8)^2+(3/8)^2=0,53$|$(0/2)^2+(2/2)^2=1,00$|$1-0,53=0,47$|$1-0,00=0,00$|$8/10*0,47+2/10*0,00=0,37$
$X_{2} \leq\ 9,5$|$(5/9)^2+(4/9)^2=0,50$|$(0/1)^2+(1/1)^2=1,00$|$1-0,50=0,50$|$1-0,00=0,00$|$9/10*0,50+1/10*0,00=0,45$

Um die Unreinheit möglichst klein zu halten, muss der Wert $p_L \cdot i(T_L) + p_R \cdot i(T_R)$ minimal sein. Dies ist gegeben, wenn man im ersten Knoten den Entscheidungswert $X_{1} \leq\ 7,5$ wählt. (Es wäre auch der selber Wert für $X_{2}$ möglich.)

Für die Menge der Datensätze, für die gilt X1 < 7,5:
Entscheidung     |$S(t_L)$              |$S(t_R)$              |$i(T_L)$     | $i(T_R)$    | $p_L \cdot i(T_L) + p_R \cdot i(T_R)$
-----------------|----------------------|----------------------|-------------|-------------|---------------------------------------
$X_{1} \leq\ 1,5$|$1^2+0^2=1$           |$(4/6)^2+(2/6)^2=0,55$|$1-1=0$      |$1-0,55=0,45$|$1/7*0,00+6/7*0,45=0,47$
$X_{1} \leq\ 2,5$|$1^2+0^2=1$           |$(3/5)^2+(2/5)^2=0,52$|$1-1,00=0,00$|$1-0,52=0,47$|$2/7*0,00+5/7*0,48=0,34$
$X_{1} \leq\ 3,5$|$(2/3)^2+(1/3)^2=0,56$|$(3/4)^2+(1/4)^2=0,62$|$1-0,56=0,44$|$1-0,62=0,38$|$3/7*0,44+4/7*0,38=0,4$
$X_{1} \leq\ 4,5$|$(3/4)^2+(1/4)^2=0,62$|$(2/3)^2+(1/3)^2=0,55$|$1-0,62=0,38$|$1-0,00=0,45$|$4/7*0,38+3/7*0,45=0,41$
$X_{1} \leq\ 5,5$|$(4/5)^2+(1/5)^2=0,68$|$(1/2)^2+(1/2)^2=0,50$|$1-0,68=0,32$|$1-0,50=0,50$|$5/7*0,32+2/7*0,50=0,37$
$X_{1} \leq\ 6,5$|$(4/6)^2+(2/6)^2=0,55$|$(1/1)^2+(0/1)^2=1,00$|$1-0,55=0,45$|$1-0,00=0,00$|$6/7*0,45+1/7*0,00=0,38$
                 |                      |                      |             |             |
$X_{2} \leq\ 1,5$|$1^2+0^2=1$           |$(4/6)^2+(2/6)^2=0,55$|$1-1=0$      |$1-0,55=0,45$|$1/7*0,00+6/7*0,45=0,38$
$X_{2} \leq\ 2,5$|$(1/2)^2+(1/2)^2=0,50$|$(4/5)^2+(1/5)^2=0,68$|$1-0,50=0,50$|$1-0,68=0,32$|$2/7*0,50+5/7*0,32=0,37$
$X_{2} \leq\ 3,5$|$(2/3)^2+(1/3)^2=0,55$|$(3/4)^2+(1/4)^2=0,62$|$1-0,55=0,45$|$1-0,62=0,38$|$3/7*0,45+4/7*0,38=0,41$
$X_{2} \leq\ 4,5$|$(3/4)^2+(1/4)^2=0,62$|$(2/3)^2+(1/3)^2=0,55$|$1-0,62=0,38$|$1-0,55=0,45$|$4/7*0,38+3/7*0,45=0,41$
$X_{2} \leq\ 5,5$|$(4/5)^2+(1/5)^2=0,68$|$(1/2)^2+(1/2)^2=0,50$|$1-0,68=0,32$|$1-0,50=0,50$|$5/7*0,32+2/7*0,50=0,37$
$X_{2} \leq\ 8  $|$(5/6)^2+(1/6)^2=0,72$|$(0/1)^2+(1/1)^2=1,00$|$1-0,72=0,38$|$1-1,00=0,00$|$6/7*0,38+1/7*0,00=0,32$

Auf Grund des niedrigen Wertes wird diese Menge an dem Entscheidungswert $X_{2} \leq\ 8$ entschieden. Die Unterteilung ist für diese Menge damit beendet, da das Kriterium erfüllt wird, dass in jedem Knoten maximal eine fremde Beobachtung enthalten ist. Dabei werden alle Werte, die den Entscheidungswert erfüllen der Klasse A und alle anderen der Klasse B zugeordnet.

Für die Datensätze, für die $X_{1} \leq\ 7,5$ NICHT gilt, müssen keine weiteren Berechnungen erfolgen, da für diesen Knoten bereits gilt, dass der Knoten rein ist. Alle Werte in diesem Knoten werden der Klasse B zugeordnet.

### b)
<!-- Auch hier ist Ihnen freisgestellt, ob sie in Latex einen Baum zeichnen möchten oder eine separate pdf-Datei abgeben -->
Siehe Anlage \textit{Aufgabe3_b.pdf}.

### c)
Die Beobachtung (7,7) wird der Klasse A zugeordnet.
Die Beobachtung (1,8) wird der Klasse A zugeordnet.
Die Beobachtung (7.5,2) wird der Klasse A zugeordnet.

## Aufgabe 4
### a)
Daten
```{r}
set.seed(1)
d <- data.frame(x1 = round(rep(c(4, 7), each = 30) + rnorm(30, sd = 2)),
               x2 = round(rep(c(3, 6, 3, 6), each = 15) + rnorm(30, sd = 2)),
               class = factor(rep(c(1, 0, 0, 1), each = 15)))

```

Modell
```{r}
cartModel <- rpart(formula = class ~ x1 + x2, method = "class", data = d)


```

Plot
```{r}
library(rpart.plot)
rpart.plot(cartModel)

```

### b)
```{r, fig.width= 6, fig.height = 6}
plot(NULL, xlim = c(0, 10), ylim=c(0, 10), xlab = expression(x[1]), ylab = expression(x[2]),
    axes = FALSE, bty = "o", cex.lab = 1.2)
box()
par(xaxp = c(0, 10, 10), yaxp = c(0, 10, 10))
axis(1, 0:10, cex.axis = 1.2)
axis(2, 0:10, cex.axis = 1.2)
grid(NULL, NULL, lwd = 1)
points(d$x1, d$x2, col = as.factor(d$class), pch = as.numeric(d$class) + 19)
legend("topleft", legend = c("0", "1"), pch = c(20, 21), col = c(1,2))

# hier die Befehele für die Eintscheidungsgrenzen hinzufügen
lines(x=c(6.5,6.5),y=c(0,10))  #x1 < 6.5
lines(x=c(0,6.5),y=c(3.5,3.5)) #x2 >= 3.5 
lines(x=c(3.5,3.5),y=c(3.5, 10)) #x1 >= 3.5
lines(x=c(6.5,10),y=c(4,4)) #x2 < 4

```



### c)
```{r}
# http://stat.ethz.ch/R-manual/R-devel/library/rpart/html/predict.rpart.html
new.data <- data.frame("x1" = c(7.0,1.0,7.5), "x2" = c(7.0,8.0,2.0))
predict(object = cartModel, newdata = new.data, type = "class")


```


