---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Lösungsvorschlag Übungsblatt 10"
author: "Alexander Kojen, Robin Czarnetzki, Jonas Kauke"  # Hier die Namen aller Gruppenmitglieder eintragen!
output: pdf_document
header-includes: 
  - \usepackage{tikz}
  - \usepackage{tikz-qtree}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
```
## Aufgabe 1

### a)
```{r}
set.seed(1234)

idx <- sample(1:nrow(iris), 50)
test <- iris[idx,]
XTest <- test[,1:4]
YTest <- as.numeric(test$Species == "virginica")

train <- iris[-idx,]
XTrain <- train[,1:4]
YTrain <- as.numeric(train$Species == "virginica")

#Gradient descent:

#A5 Eingaben:
weights <- rep(0, 4)
b <- 0
NSteps = 100
Eta = 0.1
#Eta = max( eigen( XTrain )$values )

f <- function(x) {
  return(sigmoid(as.numeric(x) %*% as.numeric(weights)+b))
}

df <- function(x) {
  return(f(x)*(1-f(x)))
}

ell <- function(x, y) {
  return (1/2 * (y - f(x))^2)
}

dell <- function(x, y) {
  err <- (y - f(x))
  return(-err*df(x))
}

sigmoid <- function(z) {
  return(1/(1+exp(-z)))
}

errs <- c()
lastError <- Inf

for(i in 1:NSteps) {
 for (j in 1:nrow(XTrain)){
   
    del <- dell(XTrain[j,], YTrain[j])
    weights <- weights + Eta * -del * XTrain[j,]
    b <- b + Eta * -del
    rss <- ell(XTrain[j,], YTrain[j])
    errs<- c(errs, rss)
 }
  avgErr <- mean(errs)
  #cat("Step ", i  ,":avg RSS: ", avgErr , "\n")

  if(lastError < avgErr){
    #cat("\n\nGets bigger!!!\n\n")
  }
  
  lastError <- avgErr
  errs <- c()
}

evaluate <- function(test, target){
  testResult <- apply(test, 1, f)
  testResult <- as.numeric(testResult > 0.9 )
  freq <- table(testResult == target)
  acc <- freq[names(freq)=="TRUE"]/ nrow(test)
  return(acc)
}

accTest <- evaluate(XTest, YTest)
accTrain <- evaluate(XTrain, YTrain)

cat( "Accuracy in test data: ", accTest, "\n")
cat( "Accuracy in train data: ", accTrain, "\n")




```


### b)

#TODO: Diskutieren Sie die Unterschiede und die Gemeinsamkeiten zwischen dem Perceptron, der SVM und der logistischen Regression!

### c)
```{r}
library(neuralnet)

set.seed(1234)

idx <- sample(1:nrow(iris), 50)
test <- iris[idx,]
train <- iris[-idx,]

# Binarize the categorical output
train <- cbind(train, train$Species == 'setosa')
train <- cbind(train, train$Species == 'versicolor')
train <- cbind(train, train$Species == 'virginica')

names(train)[6] <- 'setosa'
names(train)[7] <- 'versicolor'
names(train)[8] <- 'virginica'

neuralIris <- function(hiddenLayers){
  
  nn <- neuralnet(
    setosa+versicolor+virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
    data=train, 
    hidden=hiddenLayers
  )
  #plot(nn)
  
  comp <- compute(nn, test[-5])
  pred.weights <- comp$net.result
  idx <- apply(pred.weights, 1, which.max)
  print(pred.weights)
  pred <- c('setosa', 'versicolor', 'virginica')[idx]
  freq <- table(pred, test$Species)
  
  acc <- sum(diag(freq))/sum(freq)
  
  cat("Accuracy with hidden Layers ", hiddenLayers, " :" , acc, "\n")
}

neuralIris(c(1))
neuralIris(c(2))
neuralIris(c(5))
neuralIris(c(10))

neuralIris(c(1,1))
neuralIris(c(1,2))
neuralIris(c(1,5))
neuralIris(c(1,10))

neuralIris(c(2,1))
neuralIris(c(2,2))
neuralIris(c(2,5))
neuralIris(c(2,10))

neuralIris(c(3,1))
neuralIris(c(3,2))
neuralIris(c(3,5))
neuralIris(c(3,10))

neuralIris(c(1, 1, 1))
neuralIris(c(2, 2, 2))
neuralIris(c(5, 5, 5))
neuralIris(c(10, 10, 10))

neuralIris(c(1, 1, 10))
neuralIris(c(1, 10, 1))
neuralIris(c(10, 1, 1))

neuralIris(c(2, 2, 10))
neuralIris(c(2, 10, 2))
neuralIris(c(10, 2, 2))

neuralIris(c(5, 5, 10))
neuralIris(c(5, 10, 5))
neuralIris(c(10, 5, 5))

``` 

## Aufgabe 2

### a)
```{r}
data = list(
 read.table("blobs.csv", header=TRUE, sep = ","),
 read.table("circles.csv", header=TRUE, sep = ","),
 read.table("moons.csv", header=TRUE, sep = ","),
 read.table("random.csv", header=TRUE, sep = ",")
)

for (dataSet in data) {
    plot(dataSet)
}
```

### b)
```{r}
```

###c) 

## Aufgabe 3

```{r}
daten <- data.frame(Lebenserwartung = c(80.7, 81.8, 80.8, 83.0, 82.1),
                    Kleinkindersterblichkeitsrate = c(3.4, 3.3, 3.7, 2.1, 2.6))
names <- c("Deutschland", "Frankreich", "Irland", "Island", "Schweden")
rownames(daten) = names
```

## a)
```{r}
plot(daten, col = c("green", "red", "green", "blue", "orange"), pch = row.names(daten))
print("Erklärtung: Deutschland und Irland bilden für mich auf den ersten Blick einen Cluster, daher stimmen ihre Farben im Plot überein.")

euclDistMtx <- function (daten) {
  n <- nrow(daten)
  eDmtx <- matrix(c(rep(0,n*n)), nrow=n, ncol=n)
  dimnames(eDmtx) = list(names, names)
  
  for (i in 1:5) {
    for (j in 1:5) {
      eDmtx[i,j] <- sqrt( (daten[j,1] - daten[i,1])^2 + (daten[j,2] - daten[i,2])^2)
    }
  }
  return(eDmtx)
}
euclDist <- euclDistMtx(daten)

##########################Test##########################
as.data.frame(euclDist)
cat("\n")
probe <- dist(daten, method = "euclidean", diag = T, upper = T)
probe
########################################################

```

## b)
```{r}

```

## c)
```{r}

```

## d)
```{r}
normDaten <- data.frame(Lebenserwartung = c(rep(0,5)),
                    Kleinkindersterblichkeitsrate = c(rep(0,5)))
rownames(normDaten) = names

#Normalisierung mit mean = 0, var <- 1 (& sd = 1) macht keinen Sinn, da nicht wirklich normalisiert wird ~ Xi / 1.
#Daher: Normalisierung per https://en.wikipedia.org/wiki/Standard_score mit datenspezifischen mean & sd.
meanX <- mean(daten[,1])
meanY <- mean(daten[,2])
sdX <- sd(daten[,1])
sdY <- sd(daten[,2])
for (i in 1:nrow(daten)) {
  normDaten[i,1] <- (daten[i,1] - meanX) / sdX
  normDaten[i,2] <- (daten[i,2] - meanY) / sdY
}

normDist <- dist(normDaten, method = "euclidean", diag = T, upper = T)

singleClust <- hclust(normDist, method = "single")
completeClust <- hclust(normDist, method = "complete")

plot(singleClust)
plot(completeClust)

```

#TODO
Wie viele Cluster würden Sie nun wählen? Vergleichen Sie mit Teilaufgabe a)–c) und erklären Sie eventuelle Unterschiede.

## Aufgabe 4

### a)
```{r}

```

### b)
```{r}

```

### c)
```{r}

```
