---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Lösungsvorschlag Übungsblatt 10"
output: pdf_document
header-includes: 
  - \usepackage{tikz}
  - \usepackage{tikz-qtree}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
library("dbscan")
```
## Aufgabe 1

### a)
```{r}
set.seed(1234)

idx <- sample(1:nrow(iris), 50)
test <- iris[idx,]
XTest <- test[,1:4]
YTest <- as.numeric(test$Species == "virginica")

train <- iris[-idx,]
XTrain <- train[,1:4]
YTrain <- as.numeric(train$Species == "virginica")

#Gradient descent:

#A5 Eingaben:
weights <- rep(0, 4)
b <- 0
NSteps = 100
Eta = 0.1
#Eta = max( eigen( XTrain )$values )

f <- function(x) {
  return(sigmoid(as.numeric(x) %*% as.numeric(weights)+b))
}

df <- function(x) {
  return(f(x)*(1-f(x)))
}

ell <- function(x, y) {
  return (1/2 * (y - f(x))^2)
}

dell <- function(x, y) {
  err <- (y - f(x))
  return(-err*df(x))
}

sigmoid <- function(z) {
  return(1/(1+exp(-z)))
}

errs <- c()
lastError <- Inf

for(i in 1:NSteps) {
 for (j in 1:nrow(XTrain)){
   
    del <- dell(XTrain[j,], YTrain[j])
    weights <- weights + Eta * -del * XTrain[j,]
    b <- b + Eta * -del
    rss <- ell(XTrain[j,], YTrain[j])
    errs<- c(errs, rss)
 }
  avgErr <- mean(errs)
  #cat("Step ", i  ,":avg RSS: ", avgErr , "\n")

  if(lastError < avgErr){
    #cat("\n\nGets bigger!!!\n\n")
  }
  
  lastError <- avgErr
  errs <- c()
}

evaluate <- function(test, target){
  testResult <- apply(test, 1, f)
  testResult <- as.numeric(testResult > 0.5 )
  freq <- table(testResult == target)
  acc <- freq[names(freq)=="TRUE"]/ nrow(test)
  return(acc)
}

accTest <- evaluate(XTest, YTest)
accTrain <- evaluate(XTrain, YTrain)

cat( "Accuracy in test data: ", accTest, "\n")
cat( "Accuracy in train data: ", accTrain, "\n")




```


### b)

Alle Verfahren werden zur Klassifikation eingesetzt.
Um im Datensatz eine Abgrenzung vorzunehmen, sucht Perceptron sucht nach einer Hyperebene, während SVM den Abstand zw. Supportvektoren maximiert, um eine optimale Hyperebene zu finden, die beide Klassen trennt.
Dementsprechend ist SVM generell langsamer (Quadratische Optimierung) und lässt sich weniger leicht auf Datenflüssen anwenden (online training).
SVMs sind deterministisch, logische Regression (LR) sind dagegen probabilistisch und maximieren die Wahrscheinlichkeit, was wiederum Inferenzen auf Daten erlauben kann.

### c)
```{r}
library(neuralnet)

set.seed(1234)

idx <- sample(1:nrow(iris), 50)
test <- iris[idx,]
train <- iris[-idx,]

# Binarize the categorical output
train <- cbind(train, train$Species == 'setosa')
train <- cbind(train, train$Species == 'versicolor')
train <- cbind(train, train$Species == 'virginica')

names(train)[6] <- 'setosa'
names(train)[7] <- 'versicolor'
names(train)[8] <- 'virginica'

neuralIris <- function(hiddenLayers){
  
  nn <- neuralnet(
    setosa+versicolor+virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
    data=train, 
    hidden=hiddenLayers
  )
  #plot(nn)
  
  comp <- compute(nn, test[-5])
  pred.weights <- comp$net.result
  idx <- apply(pred.weights, 1, which.max)
  print(pred.weights)
  pred <- c('setosa', 'versicolor', 'virginica')[idx]
  freq <- table(pred, test$Species)
  
  acc <- sum(diag(freq))/sum(freq)
  
  cat("Accuracy with hidden Layers ", hiddenLayers, " :" , acc, "\n")
}

neuralIris(c(1))
neuralIris(c(2))
neuralIris(c(5))
neuralIris(c(10))

neuralIris(c(1,1))
neuralIris(c(1,2))
neuralIris(c(1,5))
neuralIris(c(1,10))

neuralIris(c(2,1))
neuralIris(c(2,2))
neuralIris(c(2,5))
neuralIris(c(2,10))

neuralIris(c(3,1))
neuralIris(c(3,2))
neuralIris(c(3,5))
neuralIris(c(3,10))

neuralIris(c(1, 1, 1))
neuralIris(c(2, 2, 2))
neuralIris(c(5, 5, 5))
neuralIris(c(10, 10, 10))

neuralIris(c(1, 1, 10))
neuralIris(c(1, 10, 1))
neuralIris(c(10, 1, 1))

neuralIris(c(2, 2, 10))
neuralIris(c(2, 10, 2))
neuralIris(c(10, 2, 2))

neuralIris(c(5, 5, 10))
neuralIris(c(5, 10, 5))
neuralIris(c(10, 5, 5))

``` 

## Aufgabe 2

### a)
```{r}
data = list(
 read.table("blobs.csv", header=TRUE, sep = ","),
 read.table("circles.csv", header=TRUE, sep = ","),
 read.table("moons.csv", header=TRUE, sep = ","),
 read.table("random.csv", header=TRUE, sep = ",")
)

for (dataSet in data) {
    plot(dataSet)
}
```

### b)
```{r}
for (dataSet in data) {
  k1 <- kmeans(x=dataSet, centers=2)
  plot(dataSet, col = k1$cluster, main="kmeans - 2")  
  k1 <- kmeans(x=dataSet, centers=3)
  plot(dataSet, col = k1$cluster, main="kmeans - 3")
  k1 <- kmeans(x=dataSet, centers=4)
  plot(dataSet, col = k1$cluster, main="kmeans - 4")
  
  db <- dbscan(x=dataSet, eps = 0.03)
  plot(x=dataSet, col = db$cluster + 1L, main="dbscan - 0.03")  
  db <- dbscan(x=dataSet, eps = 0.1)
  plot(x=dataSet, col = db$cluster + 1L, main="dbscan - 0.1") 
  db <- dbscan(x=dataSet, eps = 1.5)
  plot(x=dataSet, col = db$cluster + 1L, main="dbscan - 1.5")
}
```

###c) 
Während KMEANS die Datensätze in k Bereiche partitioniert, clustert DBScan auf Basis der Dichte. Diesen Unterschied merkt man insbesondere bei den Datensätzen circles und moons. Hier bildet DBScan die zwei Cluster bei entsprechend klein gewähltem maximalen Abstand (Radius) ekt ab.

#, während KMEANS den Datensatz immer in die fest gegebene Anzahl Cluster unterteilt# AKMEANS hingegen eignet sich besonders bei Datensätzen, die vollständig voneinander getrennt sind und sich dabei keine Bereiche innerhalb der Dimensionen "teilen", wie im ersten Datensatz blobs zu erkennen ist. Hier unterteilt KMEANS bei passender Anzahl Cluster (k=3) den Datensatz vollständig auf, ohne dass Rauschpunkte übrig bleiben, die durch ein schrittweises Anpassen wie bei DBScan erst noch eliminiert werden müssen.

DBScan eignet sich somit mehr für Datensätze, in denen die Cluster keine größere Streuung besitzen, aber sich in den Dimensionen "überschneiden" können. 

ufgabe 3

```{r}
daten <- data.frame(Lebenserwartung = c(80.7, 81.8, 80.8, 83.0, 82.1),
                    Kleinkindersterblichkeitsrate = c(3.4, 3.3, 3.7, 2.1, 2.6))
names <- c("Deutschland", "Frankreich", "Irland", "Island", "Schweden")
rownames(daten) = names
```

## a)
```{r}
plot(daten, col = c("green", "red", "green", "blue", "red"), pch = row.names(daten))
cat("Erklärtung: Deutschland und Irland sowohl als auch Frankreich und Schweden bilden für uns auf den ersten Blick je einen Cluster, daher stimmen ihre Farben im Plot je überein. Island stellt einen Cluster für sich dar.", "\n")

cat("\n", "Euklidische Distanz:", "\n")

euclDistMtx <- function (daten) {
  n <- nrow(daten)
  eDmtx <- matrix(c(rep(0,n*n)), nrow=n, ncol=n)
  dimnames(eDmtx) = list(names, names)
  
  for (i in 1:5) {
    for (j in 1:5) {
      eDmtx[i,j] <- sqrt( (daten[j,1] - daten[i,1])^2 + (daten[j,2] - daten[i,2])^2)
    }
  }
  return(eDmtx)
}
euclDist <- euclDistMtx(daten)

as.data.frame(euclDist)
##########################Test##########################
cat("\n", "Probe mit Dist-Funktion: ", "\n")
probe <- dist(daten, method = "euclidean", diag = T, upper = T)
probe
########################################################

```

## b)
```{r}

##########################Für Teil d)##########################
singleClust <- hclust(probe, method = "single")
completeClust <- hclust(probe, method = "complete")

plot(singleClust)
plot(completeClust)
########################################################


```

## c)
```{r}

```

## d)
```{r}
normDaten <- data.frame(Lebenserwartung = c(rep(0,5)),
                    Kleinkindersterblichkeitsrate = c(rep(0,5)))
rownames(normDaten) = names

#Normalisierung mit mean = 0, var <- 1 (& sd = 1) macht keinen Sinn, da nicht wirklich normalisiert wird ~ Xi / 1.
#Daher: Normalisierung per https://en.wikipedia.org/wiki/Standard_score mit datenspezifischen mean & sd.
meanX <- mean(daten[,1])
meanY <- mean(daten[,2])
sdX <- sd(daten[,1])
sdY <- sd(daten[,2])
for (i in 1:nrow(daten)) {
  normDaten[i,1] <- (daten[i,1] - meanX) / sdX
  normDaten[i,2] <- (daten[i,2] - meanY) / sdY
}

normDist <- dist(normDaten, method = "euclidean", diag = T, upper = T)

singleClust <- hclust(normDist, method = "single")
completeClust <- hclust(normDist, method = "complete")

plot(singleClust)
plot(completeClust)

```

Wir würden 3 Cluster wählen ({ISL}, {GER, IRL}, {FRA, SWE})
In Teilaufgabe a) waren auf den ersten Blick 4 Cluster erkennbar, wobei nur Deutschland und Irland einen vereinigten Cluster bildeten.
In b) legten die Dendrogramme nahe, dass Island, Frankreich und Schweden auch einen Cluster bilden, wobei die letzten beiden einen Cluster für sich bilden.
Mit normalisierten Daten in d) bildet Island einen individuellen Cluster, wohingegen Deutschland & Irland als auch Frankreich und Schweden je einen gemeinsamen Cluster bilden, der in einem Obercluster (...) aus den Vieren mündet.
Der Unterschied ist darauf zurückzuführen, dass die Lebenserwartung (80.7 bis 83) im Vgl. zur Kleinkindersterblichkeitsrate (2.1 bis 3.7) eine geringere Streuung aufwies. Durch die Normalisierung wird dieser Unterschied kaschiert, sodass die intuitive Cluster-Einteilung in a) den Ergebnissen von d) näher kommt als denen aus b) & c).

## Aufgabe 4

### a)
```{r}

```

### b)
```{r}

```

### c)
```{r}

```
