---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Übungsblatt 09"
author: "Alexander Kojen, Robin Czarnetzki, Jonas Kauke"  # Hier die Namen aller Gruppenmitglieder eintragen!
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
library("MASS")
library("mlr")
library("mlbench")
library("caTools")
        # die Pakete muessen gegebenenfalls installiert werden!
```

## Aufgabe 1

### a)
Learner
```{r}
# Wenden Sie die folgenden Lernverfahren mit mlr auf die Datensätze an: Naive Bayes,
# kNN mit k = 3 und mit k = 21, SVM mit linearem und mit radialem Kern, AdaBoost, Entscheidungsbaum
# und Random Forest mit 5 und mit 500 Bäumen. Visualisieren Sie die Entscheidungsgrenzen
# (plotLearnerPrediction)

learners = list(
  makeLearner("classif.naiveBayes"),
  makeLearner("classif.knn", k = 3),
  makeLearner("classif.knn", k = 21),
  makeLearner("classif.svm", kernel = "linear"),
  makeLearner("classif.svm", kernel = "radial"),
  makeLearner("classif.rpart"),
  makeLearner("classif.randomForest", ntree=5),
  makeLearner("classif.randomForest", ntree=500)
  #makeLearner("classif.adaboostm1")
)

```
Tasks
```{r}
tasks = list(
  makeClassifTask(id = "2dnormals", target = "classes", data = do.call(data.frame, mlbench.2dnormals(500, 2))),
  makeClassifTask(id = "smiley", target = "classes", data = do.call(data.frame, mlbench.smiley(500, 0.1, 0.05))),
  makeClassifTask(id = "cassini", target = "classes", data = do.call(data.frame, mlbench.cassini(5000)))
)
```


Plots
```{r, fig.pos = "p", fig.height = 4}
set.seed(1273)
for (tsk in tasks) {
  for (lrn in learners) {
    print(plotLearnerPrediction(lrn, task = tsk))
  }
}
```


### b)




## Aufgabe 2
```{r}
S <- matrix(c(5, 2, 2, 2), nrow = 2)
```

### a)
```{r}
eigenVal <- eigen(S)$values
# Mit diesen Eigenwerten ergeben sich die Eigenvektoren
eigenVec <- cbind (c(-1,2), c(2,1))
Z <- eigenVec %*% S
varZ <- var(Z)


print(eigenVal)
print(eigenVec)
print(Z)
```
Die Loadings lauten
$$ 
g_1 = \begin{pmatrix}-1\\2\end{pmatrix} \\
g_2 = \begin{pmatrix}2\\1\end{pmatrix}
$$

Die Hauptkomponenten lauten
$$ 
z_1 = \begin{pmatrix}-1\\2\end{pmatrix} \\
z_2 = \begin{pmatrix}12\\6\end{pmatrix}
$$
Der Anteil der durch die erste Hauptkomponente erklärten Varianz ergibt sich durch:
```{r}
rP1 <- sum( varZ[1,] )
rK <- sum( varZ )
anteil1 <- rP1/rK

cat("\nAnteil der ersten Hauptkomponente ist ", anteil1, "\n") # oder hier schon geordnet?!

```




### b)
Korrelationsmatrix
```{r}

#corr <- cor(S) # geht nicht
```


```{r}
```
Hauptkomponenten bzw. Scores $z_1$ und $z_2$:
$$
z_1 = 
$$
$$
z_2 = 
$$

Varianzanteil
```{r}
#Irgendwo im A2 kann man wohl Funktion cov2cor nutzen oder(?):
# Cov / wurzel (produkt der varianzen)
```

### c)


## Aufgabe 3
Einlesen der Daten
```{r}
daten <- read.table("bank.txt")
```


### a)
HKA auf Basis von Kovarianzen:
```{r}
daten.num <- daten[,1:6]
daten.class <- daten[,7]

pca_cov <- prcomp(daten.num, scale. = F) # in man: princomp(USArrests, cor = TRUE) # =^= prcomp(USArrests, scale=TRUE)

```

HKA auf Basis von Korrelationen:
```{r}

pca_cor <- prcomp(daten.num, scale. = T)

```

### b)

Screeplot
```{r}

screeplot(pca_cov, type="l")
screeplot(pca_cor, type="l")

```

Anteil an der Gesamtvarianz
```{r}
summary(pca_cov)
summary(pca_cor)
```

Nach der HKA auf Basis der Kovarianzmatrix ergibt sich eine Reduktion auf die ersten 3 Hauptkomponenten, da hier der Screeplot abflacht und bereit 93% der Varianz ausmachen. Auf Basis der Korrelationsmatrix würde man allerdings nur auf 4 Komponenten reduzieren, da hier erst eine Gesamtvariant von 93% erreicht wird.

# TODO: Für welche der beiden Hauptkomponentenanalysen aus Teil a) sollten die Loadings überhaupt interpretiert werden? Interpretieren Sie die entsprechenden Loadings der ersten Hauptkomponente

### c)
Biplot
```{r}

biplot(pca_cov, main="COV")
biplot(pca_cor, main="COR")

```


Interpretation:
# TODO: Welche Scores-Struktur liegt vor? Vergleichen und interpretieren Sie die Bi-Plots.

## Aufgabe 4

### a)
Es handelt sich dabei um autoregressive Prozesse 1-ter Ordnung.
Dabei sind, aufgrund der Vorraussetzung von |ß_2| < 1 (F. 396, Zeitreihenanalyse.pdf), der 1. Prozess stationär, der 2. Prozess dagegen nicht.

### b)
Funktion zum Simulieren
```{r}
proc1 <- c(1, -0.9)
proc2 <- c(-0.2, 1.25)
errors <- rnorm(0,0.5, n = 500)
```

Simulation
```{r}
timeSeriesSim <- function(process, err) {
  n <- length(err)
  beta1 <- process[1]
  beta2 <- process[2]
  yt <- c(0) #yt1: 0 quasi als Startparameter, da noch kein yt-1 da ist.
  for (i in 2:n) {
    yt <- c(yt,beta1+beta2*yt[i-1]+err[i])
  }
  return(yt)
}

ts1 <- timeSeriesSim(proc1, errors)
ts2 <- timeSeriesSim(proc2, errors)

#ts1 <- arima.sim(n = 500, list(ar = proz1), sd = sqrt(0.5))
#ts2 <- arima.sim(n = 500, list(ar = proz2), sd = sqrt(0.5))

```

Darstellung
```{r}
#F. 398:
expVal1 <- 1 / (1-(-0.9))
plot1 <- plot(1:500, y = ts1, type = "l")
abline(h =expVal1, col = "green")
plot2 <- plot(1:500, y = ts2, type = "l")
```


### c)
Glätten mit einem einfachen gleitenden Durchschnitt
```{r}
runmean1 <- runmean(x = ts1, k = 20)
plot1 <- plot(1:500, y = ts1, type = "l")
abline(h =expVal1, col = "green")
lines(runmean1, col = "red")

runmean2 <- runmean(x = ts2, k = 20)
plot2 <- plot(1:500, y = ts2, type = "l")
lines(runmean2, col = "red")
```




