---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Übungsblatt 09"
author: "Alexander Kojen, Robin Czarnetzki, Jonas Kauke"  # Hier die Namen aller Gruppenmitglieder eintragen!
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
library("MASS")
library("mlr")
library("mlbench")
library("caTools")
        # die Pakete muessen gegebenenfalls installiert werden!
```

## Aufgabe 1

### a)
Learner
```{r}
# Wenden Sie die folgenden Lernverfahren mit mlr auf die Datensätze an: Naive Bayes,
# kNN mit k = 3 und mit k = 21, SVM mit linearem und mit radialem Kern, AdaBoost, Entscheidungsbaum
# und Random Forest mit 5 und mit 500 Bäumen. Visualisieren Sie die Entscheidungsgrenzen
# (plotLearnerPrediction)

#Daten:
normals <- mlbench.2dnormals(500, 2)
circles <- mlbench.circle(500, 2)
spirals <- mlbench.spirals(500, cycles = 2)


```
Tasks
```{r}
#to do: Wie in älteren Übungen. Tasks & Learner in liste packen und dann (mit Schleife?) plotten 
normalsTask = makeClassifTask(data = normals, target = "Class")
model = train(makeLearner("classif.naiveBayes"), task)

```


Plots
```{r, fig.pos = "p", fig.height = 4}

```


### b)




## Aufgabe 2
```{r}
S <- matrix(c(5, 2, 2, 2), nrow = 2)
```

### a)
```{r}
```
Die Hauptkomponenten lauten
$$
z_1 = 
$$
$$
z_2 =
$$
Der Anteil der durch die erste Hauptkomponente erklärten Varianz ergibt sich durch:
```{r}

```




### b)
Korrelationsmatrix
```{r}


```


```{r}
```
Hauptkomponenten bzw. Scores $z_1$ und $z_2$:
$$
z_1 = 
$$
$$
z_2 = 
$$

Varianzanteil
```{r}
#Irgendwo im A2 kann man wohl Funktion cov2cor nutzen oder(?):
# Cov / wurzel (produkt der varianzen)
```

### c)


## Aufgabe 3
Einlesen der Daten
```{r}
daten <- read.table("bank.txt")
```

### a)
HKA auf Basis von Kovarianzen:
```{r}

```

HKA auf Basis von Korrelationen:
```{r}

```

### b)

Screeplot
```{r}

```

Anteil an der Gesamtvarianz
```{r}

```

### c)
Biplot
```{r}

```


Interpretation:

## Aufgabe 4

### a)
Es handelt sich dabei um autoregressive Prozesse 1-ter Ordnung.
Dabei sind, aufgrund der Vorraussetzung von |ß_2| < 1 (F. 396, Zeitreihenanalyse.pdf), der 1. Prozess stationär, der 2. Prozess dagegen nicht.

### b)
Funktion zum Simulieren
```{r}
proc1 <- c(1, -0.9)
proc2 <- c(-0.2, 1.25)
errors <- rnorm(0,0.5, n = 500)
```

Simulation
```{r}
timeSeriesSim <- function(process, err) {
  n <- length(err)
  beta1 <- process[1]
  beta2 <- process[2]
  yt <- c(0) #yt1: 0 quasi als Startparameter, da noch kein yt-1 da ist.
  for (i in 2:n) {
    yt <- c(yt,beta1+beta2*yt[i-1]+err[i])
  }
  return(yt)
}

ts1 <- timeSeriesSim(proc1, errors)
ts2 <- timeSeriesSim(proc2, errors)

#ts1 <- arima.sim(n = 500, list(ar = proz1), sd = sqrt(0.5))
#ts2 <- arima.sim(n = 500, list(ar = proz2), sd = sqrt(0.5))

```

Darstellung
```{r}
#F. 398:
expVal1 <- 1 / (1-(-0.9))
plot1 <- plot(1:500, y = ts1, type = "l")
abline(h =expVal1, col = "green")
plot2 <- plot(1:500, y = ts2, type = "l")
```


### c)
Glätten mit einem einfachen gleitenden Durchschnitt
```{r}
runmean1 <- runmean(x = ts1, k = 20)
plot1 <- plot(1:500, y = ts1, type = "l")
abline(h =expVal1, col = "green")
lines(runmean1, col = "red")

runmean2 <- runmean(x = ts2, k = 20)
plot2 <- plot(1:500, y = ts2, type = "l")
lines(runmean2, col = "red")
```




