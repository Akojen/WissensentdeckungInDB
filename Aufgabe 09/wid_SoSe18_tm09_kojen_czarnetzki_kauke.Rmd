---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Übungsblatt 09"
author: "Alexander Kojen, Robin Czarnetzki, Jonas Kauke"  # Hier die Namen aller Gruppenmitglieder eintragen!
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
library("MASS")
library("mlr")
library("mlbench")
library("caTools")
        # die Pakete muessen gegebenenfalls installiert werden!
```

## Aufgabe 1

### a)
Learner
```{r}
# Wenden Sie die folgenden Lernverfahren mit mlr auf die Datensätze an: Naive Bayes,
# kNN mit k = 3 und mit k = 21, SVM mit linearem und mit radialem Kern, AdaBoost, Entscheidungsbaum
# und Random Forest mit 5 und mit 500 Bäumen. Visualisieren Sie die Entscheidungsgrenzen
# (plotLearnerPrediction)

#Daten:
normals <- mlbench.2dnormals(500, 2)
circles <- mlbench.circle(500, 2)
spirals <- mlbench.spirals(500, cycles = 2)


```
Tasks
```{r}
#to do: Wie in älteren Übungen. Tasks & Learner in liste packen und dann (mit Schleife?) plotten 
normalsTask = makeClassifTask(data = normals, target = "Class")
model = train(makeLearner("classif.naiveBayes"), task)

```


Plots
```{r, fig.pos = "p", fig.height = 4}

```


### b)




## Aufgabe 2
```{r}
S <- matrix(c(5, 2, 2, 2), nrow = 2)
```

### a)
```{r}
eigenVal <- eigen(S)$values
# Mit diesen Eigenwerten ergeben sich die Eigenvektoren
eigenVec <- cbind (c(-1,2), c(2,1))
Z <- eigenVec %*% S
varZ <- var(Z)


print(eigenVal)
print(eigenVec)
print(Z)
```
Die Loadings lauten
$$ 
g_1 = \begin{pmatrix}-1\\2\end{pmatrix} \\
g_2 = \begin{pmatrix}2\\1\end{pmatrix}
$$

Die Hauptkomponenten lauten
$$ 
z_1 = \begin{pmatrix}-1\\2\end{pmatrix} \\
z_2 = \begin{pmatrix}12\\6\end{pmatrix}
$$
Der Anteil der durch die erste Hauptkomponente erklärten Varianz ergibt sich durch:
```{r}
rP1 <- sum( varZ[1,] )
rK <- sum( varZ )
anteil1 <- rP1/rK

cat("\nAnteil der ersten Hauptkomponente ist ", anteil1, "\n") # oder hier schon geordnet?!

```




### b)
Korrelationsmatrix
```{r}
cor <- cov2cor(S)

eigenVal_ <- eigen(cor)$values
eigenVec_ <- eigen(cor)$vector
Z_ <- eigenVec_ %*% cor
varZ_ <- var(Z_)

print(Z_)
```


```{r}
```
Hauptkomponenten bzw. Scores $z_1$ und $z_2$:
$$
z_1 = \begin{pmatrix}0.2598932\\-0.2598932\end{pmatrix}
$$
$$
z_2 = \begin{pmatrix}1.1543204\\1.1543204\end{pmatrix}
$$

Varianzanteil
```{r}
rP2 <- sum( varZ_[1,] )
rK2 <- sum( varZ_ )
anteil2 <- rP2/rK2

cat("\nAnteil der ersten Hauptkomponente ist ", anteil2, "\n")

```

### c)


## Aufgabe 3
Einlesen der Daten
```{r}
daten <- read.table("bank.txt")
```


### a)
HKA auf Basis von Kovarianzen:
```{r}
daten.num <- daten[,1:6]
daten.class <- daten[,7]

#pca_cov <- prcomp(daten.num, scale. = F)
pca_cov <- princomp(daten.num, cor = FALSE)

```

HKA auf Basis von Korrelationen:
```{r}

#pca_cor <- prcomp(daten.num, scale. = T)
pca_cor <- princomp(daten.num, cor = TRUE)

```

### b)

Screeplot
```{r}

screeplot(pca_cov, type="l")
screeplot(pca_cor, type="l")

```

Anteil an der Gesamtvarianz
```{r}
summary(pca_cov)
summary(pca_cor)

#Loadings von HKA1:
pca_cov$loadings

```

Nach der HKA auf Basis der Kovarianzmatrix ergibt sich eine Reduktion auf die ersten 3 Hauptkomponenten, da hier der Screeplot abflacht und bereits ~93% der Varianz ausmacht. Auf Basis der Korrelationsmatrix würde man allerdings nur auf 4 Komponenten reduzieren, da hier erst eine Gesamtvarianz von ~93% erreicht wird.

Laut Folie 337 werden für Loadings implizit unkorrelierte Merkmale angenommen. Daher werden wir sie für HKA1 interpretieren.
Loadings bilden die Korrelation zw. dem jew. Merkmal und der HK ab.
Demnach ist die Korrelation von HK1 und "Length" verschwindend gering. Eine leichte neg. Korrelation besteht zu den "Height" Merkmalen und zu "Inner.Frame.upper".
"Inner.Frame.lower" ist negativ, "Diagonal" ist positiv mit HK1 stark bis mittelmäßig korreliert.
Loadings werden im späteren Biplot als rote Pfeilvektoren abgebildet.


### c)
Biplot
```{r}
###################### Interpretationshilfen ######################
#Struktur
plot(pca_cov$scores[,1], pca_cov$scores[,2],col=c('red', 'green')[as.numeric(daten.class)], xlab = "Comp.1", ylab = "Comp.2", main="COV Scores") #green = genuine / red = counterfeit
plot(pca_cor$scores[,1], pca_cor$scores[,2],col=c('red', 'green')[as.numeric(daten.class)], xlab = "Comp.1", ylab = "Comp.2", main="COR Scores") #green = genuine / red = counterfeit

#Korrelation von Daten und deren 2 HKn mit dem meisten Einfluss (hilfreich bei der Interpretation von biplots):
cor(daten.num, pca_cov$scores[,1:2])
cor(daten.num, pca_cor$scores[,1:2])
###################################################################

#Plot-Befehle in die Konsole eingeben & dann zoomen...
biplot(pca_cov, main="COV", scale = 0)
biplot(pca_cor, main="COR", scale = 0)

```


Interpretation:
Die Scoresstruktur ist in beiden Fällen "interessant" (vgl. F 359, Stetige Modelle Dimensionsreduktion.pdf)...
So sind die Punktwölkchen der jew. Klassen, bis auf wenige Ausnahmen, relativ klar voneinander abgegrenzt, bilden also sog. Cluster. Zugleich weisen ihre räumlichen Ausdenungen eine gewisse Symetire auf. Beim Ausmaß der Streuung weisen die Klassen lediglich in HKA der Kovarianzen einen merklichen Unterschied auf.

Biplot 1 (HKA auf Basis von Kovarianzen):
Die Merkmale "Inner.Frame.lower" "Inner.Frame.upper" und "Diagonal" korrelieren nicht bzw. kaum miteiandner. "Height.left" und "Height.right" dagegen schon, korrelieren allerdings kaum mit den HKn. Gleiches gilt auch für "Length".
Der Plot liefert keine nenennswerten Erkenntnisse.

Biplot 1 (HKA auf Basis von Korrelationen):
Die Merkmale "Length" und "Diagonal" korrlieren kaum miteinander. "Height.left" und "Height.right" korrelieren dagegen recht stark miteinander. Das Gleiche gilt in abgeschwächter Form auch für  "Inner.Frame.lower" und "Inner.Frame.upper".
Dementsprechend sind sowohl die Höhe auf beiden Seiten der Banknote, als auch die Abstände der inneren Rahmens zur unteren und oberen Grenze, jeweils in ihren beiden Ausprägungen zusammen, hilfreich dabei gefälschte Banknoten von echten zu unterscheiden.


## Aufgabe 4

### a)
Es handelt sich dabei um autoregressive Prozesse 1-ter Ordnung.
Dabei sind, aufgrund der Vorraussetzung von |ß_2| < 1 (F. 396, Zeitreihenanalyse.pdf), der 1. Prozess stationär, der 2. Prozess dagegen nicht.

### b)
Funktion zum Simulieren
```{r}
proc1 <- c(1, -0.9)
proc2 <- c(-0.2, 1.25)
errors <- rnorm(0,0.5, n = 500)
```

Simulation
```{r}
timeSeriesSim <- function(process, err) {
  n <- length(err)
  beta1 <- process[1]
  beta2 <- process[2]
  yt <- c(0) #yt1: 0 quasi als Startparameter, da noch kein yt-1 da ist.
  for (i in 2:n) {
    yt <- c(yt,beta1+beta2*yt[i-1]+err[i])
  }
  return(yt)
}

ts1 <- timeSeriesSim(proc1, errors)
ts2 <- timeSeriesSim(proc2, errors)

#ts1 <- arima.sim(n = 500, list(ar = proz1), sd = sqrt(0.5))
#ts2 <- arima.sim(n = 500, list(ar = proz2), sd = sqrt(0.5))

```

Darstellung
```{r}
#F. 398:
expVal1 <- 1 / (1-(-0.9))
plot1 <- plot(1:500, y = ts1, type = "l")
abline(h =expVal1, col = "green")
plot2 <- plot(1:500, y = ts2, type = "l")
```


### c)
Glätten mit einem einfachen gleitenden Durchschnitt
```{r}
runmean1 <- runmean(x = ts1, k = 20)
plot1 <- plot(1:500, y = ts1, type = "l")
abline(h =expVal1, col = "green")
lines(runmean1, col = "red")

runmean2 <- runmean(x = ts2, k = 20)
plot2 <- plot(1:500, y = ts2, type = "l")
lines(runmean2, col = "red")
```




