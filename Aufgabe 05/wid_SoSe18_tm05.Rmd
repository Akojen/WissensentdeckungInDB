---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Übungsblatt 04"
author: "Sebastian Buschjäger, Malte Jastrow"  # Hier die Namen aller Gruppenmitglieder eintragen!
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
```
## Aufgabe 1

### a)

$$
\mathrm{f}(X) = \sum_{x\in A} x*f(x) = 0*0.5 + 1*0.5 = 0.5
$$

### b)
```{r}
modelfunc <- function(x, a, b, c) { # Modellfunktion implementiert
  c_part <- c
  a_part <- sum(a*as.numeric(x))
  
  b_part <- 0
  for( i in 1:length(x) ){
    for( j in i:length(x)){ 
        b_part = b_part + (b[i,j] * as.numeric(x[i]) * as.numeric(x[j]))
    }
  }
  return(sum(b_part, c_part, a_part))
}

modelfunc_gradient <- function(x, a, b, c){ # Gradient der Modellfunktion als Vektor ( 3 Werte )
  d_c <- 1
  d_a <- x
  d_b <- matrix(0, nrow=13, ncol= 13)
  for( i in 1:length(x) ){
    for( j in i:length(x)){
        d_b[i,j] = (as.numeric(x[i]) * as.numeric(x[j])) # neue b-Matrix auffüllen
    }
  }
  return( c(d_c, d_a, d_b) )
}

loss <- function(X, Y, a, b, c){ # Fehlerfunktion l(c,a,b,D)
  sumDiff <- 0
  for( i in 1:nrow(X) ){
    sumDiff = sumDiff + ( as.numeric(Y[i]) - modelfunc(X[i,], a, b, c) )^2
  }
  return(0.5 * sumDiff)
}

loss_gradient <- function(X, Y, a, b, c){ # Gradient der Fehlerfunktion, übernommen aus Musterlösung, hier liegt der Fehler?!
  model <- apply(X, 1, modelfunc, a=a, b=b, c=c)
  model_gradient <- matrix( apply(X, 1, modelfunc_gradient, a=a, b=b, c=c), ncol=length(c(c,a,b)))
  gradient <- -(as.numeric(Y)-model) * model_gradient
  newParam <- apply(gradient, 2, mean)
}

decent_gradient <- function(X, Y, steps, stepsize){ # führt die Anzahl der Schritte durch und berechnet Fehler und neue Paramter für a,b,c
  # Initialisieren der Darstellung der Parameter
  # im folgenden in gleicher Reihenfolge wie in Modellformel: c,a,b
  c <- 0
  a <- c(rep(0,13))
  b <- matrix(0, byrow=T, ncol=13, nrow=13)
  
  # Parameter schrittweise in Gradientenrichtung bewegen
  for( i in 1:steps){
    #G radient von aktuellem Wert um Schrittweite(stepsize) "absteigen"
    gradients <- loss_gradient(X,Y,a,b,c)
    newParam <- c(c,a,b) - stepsize*gradients
    
    # daraus neue Parameter berechnen
    c <- newParam[1]
    a <- newParam[2:14]
    b <- matrix( newParam[15:length((newParam))], ncol=13, nrow=13)
    
    #Verlust berechnen
    current_loss <- loss(X, Y, a, b, c)
    print( current_loss )
  }
  
  return( loss(X, Y, a, b, c) )
}

data <- read.table("housing.csv", header=TRUE, sep = ",")
X <- data[1:13]
Y <- unlist(data[14])
 
dg_100 <- decent_gradient(X, Y, 100, 0.0002)

```

### c)

### d)

### e)


## Aufgabe 2

### a)
```{r}
library(plot3Drgl)

# Definition der Ebenengleichung
beta <- c(-1,1,-1)
beta0 <- 1

# Beobachtungen erstellen
X <- t(replicate(500,runif(3, min=-1, max=1)))
Y <- X %*% beta + beta0

# Label erstellen
Y_ <- apply(Y, 1, function(x) {if (x > 0) { "red" } else {"blue"}})
```


### b)
```{r}
library(e1071)
```


## Aufgabe 3
```{r}


```
