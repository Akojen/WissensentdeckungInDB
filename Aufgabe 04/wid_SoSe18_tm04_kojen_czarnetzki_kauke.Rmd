---
title: "Wissensentdeckung in Datenbanken SoSe 2018"
subtitle: "Übungsblatt 04"
author: "Alexander Kojen, Robin Czarnetzki, Jonas Kauke"  # Hier die Namen aller Gruppenmitglieder eintragen!
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Code wird ausgegeben
library(mlr)
library(mlbench)
library(MASS)
library(klaR)        # die Pakete muessen gegebenenfalls installiert werden!
```
## Aufgabe 1

### a)
```{r}
data(Ionosphere)
set.seed(1273)

##a

#Split data 80% train / 20% test: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function/31634462#31634462

sample <- sample.int(n = nrow(Ionosphere), size = floor(.8*nrow(Ionosphere)), replace = F)
train <- Ionosphere[sample, ]
test  <- Ionosphere[-sample, ]

task = makeClassifTask(data = train, target = "Class")

```

### b)
```{r}
learner = makeLearner("classif.naiveBayes")
model = train(learner, task)

```

### c)
```{r}
prediction = as.data.frame(predict(model, newdata = test))

prediction_eval <- function (pred, outcome) {
  n <- nrow(pred)
  goodness <- 0
  for (i in 1:n) {
    if (pred[i,1] == pred[i,2]) {
      goodness <- goodness + 1
    }
  }
  if (outcome == "succ") {
    return(goodness / n)  
  }
  else if (outcome == "err")
    return((n - goodness) / n)
  else
    return("Does not compute.")
}

prediction_eval(prediction, "succ")
#Alternative Ausgabe ueber (mit Daten der Klasse Prediction, mmce: mean misclassification error / acc: accuracy):
performance(predict(model, newdata = test), measures = list(mmce, acc))
```

Interpretation:
Mit fast 86% trifft die vorhergesagte Klasse mit der wahren Klasse überein. Daher ist das Ergebniss zufriedenstellend.

## Aufgabe 2

### a)
```{r}
# https://www.rdocumentation.org/packages/caret/versions/6.0-78/source

train.mylda <- function(data, target) {
  targetIndex <- grep(target, colnames(data))
  
  classes <- unique(data[, targetIndex])
  cleanData <- data[,-targetIndex] #Ohne die Target-Spalte
  p <- ncol(cleanData) #Abzueglaich der Target-Spalte
  n <- c()
  meanList <- c()
  
  matSum <- matrix(0, ncol=p, nrow=p)
    
  for (i in 1: length(classes)) {
    classSpecificData <- data[data[,targetIndex] == toString(classes[i]),]
    cleanClassSpecificData <- classSpecificData[, -targetIndex]
    n <- c(n,nrow(cleanClassSpecificData))
    meanVec <- apply(cleanClassSpecificData, 2, mean)
    meanList <- c(meanList, meanVec)
  
    for (j in 1:n[i]){
      mat <- as.matrix(cleanClassSpecificData[j,] - meanVec)
      matGes <- t(mat) %*% mat
      matSum <- matSum+matGes
    }
  }
  
  meanMat <- matrix(meanList, ncol=p)
  meanFrame <- data.frame(meanMat, classes)
  colnames(meanFrame) <- colnames(data)
  
  firstPartCov <- (1/ (sum(n)-length(classes)))
  covMat <- firstPartCov*matSum
  
  return( list("cov"=covMat, "means"=meanFrame, "apriori"=n/sum(n)) )
}

```


### b)
```{r}
predict.mylda <- function(model, newdata) {
  #for (i in 1:nrow(newdata)){
  for (i in 1:1){
    invCov <- solve(model$cov)
    for (j in 1:nrow(model$means)){
      classSpecificMeanVector <- as.numeric(model$means[j,-(ncol(model$means))])
      
      first <- t(invCov*classSpecificMeanVector)*as.numeric(newdata[j,])
      second <- 0.5*classSpecificMeanVector*invCov*classSpecificMeanVector # zwei mal ohne transformation?
      third <- log( model$apriori[j] , base = exp(1))
      ges <- first-second+third
      print(ges)
    }    
  }
}
```

### c)

80/20 Aufteilungen des Iris-Datensatzes
```{r}
set.seed(201805)

samples = sample(1:150, 120) # 80/20 Aufteilung, statt 60/40

train = iris[samples,]
test = iris[-samples,]

```


Modell, Vorhersage und Fehlerrate
```{r}

model <- train.mylda(train, "Species")
prediction <- predict.mylda(model, test[,1:4])

#print(model)
#print(prediction)

```


## Aufgabe 3
### a)
```{r}
task = makeClassifTask(data = train, target = "Species")
learner = makeLearner("classif.lda")
model = train(learner, task)

prediction = as.data.frame(predict(model, newdata = test))
prediction_eval(prediction, "err")

## To do
# Vergleichen Sie die Fehlklassifikationsrate mit Ihrer eigenen Implementierung. Hätte man sich diesen Vergleich sparen können?

```

Interpretation:

### b)
```{r}
task = makeClassifTask(data = train, target = "Species")
learner = makeLearner("classif.qda")
model = train(learner, task)

prediction = as.data.frame(predict(model, newdata = test))
prediction_eval(prediction, "err")

```
Interpretation:
Die Fehlklassifikationsrate hat sich halbiert. Nur in einem von 60 Fällen entspricht die Vorhersage nicht der Wahrheit.


### c)
```{r}
task = makeClassifTask(data = train, target = "Species")

rda_param_eval <- function (gam, lam) {
  if (length(gam) != length(lam)) {
    return("Vectors of lamda and gamma params need to be equally long")
  }
  results = matrix(0, nrow = 36, ncol = 3)
  colnames(results) = c("Gamma", "Lambda", "Fehlklassifikationsrate")
  n <- length(gam)
  tabIndx <- 0
  for (i in 1:n) {
    for (j in 1:n) {
      tabIndx <- tabIndx + 1
      learner = makeLearner("classif.rda", crossval = FALSE, gamma = gam[i], lambda = lam[j])
      model <- train(learner, task)
      pred <- predict(model, newdata = test)
      results[tabIndx,1] <- gam[i]
      results[tabIndx,2] <- lam[j]
      results[tabIndx,3] <- performance(pred, measures = mmce)
    }
  }
  #results[order("Fehlklassifikationsrate"),] #Klappt nicht..
  return(results)
}

gamma <- c(0, 0.2, 0.4, 0.6, 0.8, 1)
lambda <- c(0, 0.2, 0.4, 0.6, 0.8, 1)
rda_param_eval(gamma, lambda)
```

Anwedung der RDA mit optimalen Parametern
```{r}
task = makeClassifTask(data = train, target = "Species")
learner = makeLearner("classif.rda", crossval = FALSE, gamma = 0, lambda = 0)
model = train(learner, task)

prediction = as.data.frame(predict(model, newdata = test))
prediction_eval(prediction, "err")

```



Interpretation:

Die RDA erreicht die geringste Fehlklassifikationsrate von 1,6% für mehere Parameterkombinationen, u.A. auch fuer Lambda = 0, Gamma = 0.
Dieses Ergebnis wird auch von der QDA erreicht. Daher lohnt sich der Aufwand fuer RDA nicht.


## Aufgabe 4
### a)
Learner anlegen
```{r}
learnerNBay = makeLearner("classif.naiveBayes")
learnerLDA = makeLearner("classif.lda")
learnerQDA = makeLearner("classif.qda")
learnerRDA = makeLearner("classif.rda", crossval = FALSE, gamma = 0, lambda = 0)
#To do:  Setzen Sie moegliche Parameter der Verfahren sinnvoll. Ggf. durch rda_param_eval laufen lassen? Sonst qda = rda

learnerList <- list(learnerNBay, learnerLDA, learnerQDA, learnerRDA)
```

Tasks anlegen
```{r}
# ftp://cran.r-project.org/pub/R/web/packages/mlbench/mlbench.pdf
data2dnormals <- as.data.frame(mlbench.2dnormals(500,2))
dataSmiley <- as.data.frame(mlbench.smiley(500, 0.1, 0.05))
dataCassini <- as.data.frame(mlbench.cassini(5000))

task2dnormals = makeClassifTask(data = data2dnormals, target = "classes")
taskSmiley = makeClassifTask(data = dataSmiley, target = "classes")
taskCassini = makeClassifTask(data = dataCassini, target = "classes")

taskList <- list(task2dnormals, taskSmiley, taskCassini)

```

Grafiken erzeugen
```{r, fig.pos = "p", fig.height = 4}

plotLearnerPrediction(learner = learnerNBay, task = task2dnormals)
plotLearnerPrediction(learner = learnerLDA, task = task2dnormals)
plotLearnerPrediction(learner = learnerQDA, task = task2dnormals)
plotLearnerPrediction(learner = learnerRDA, task = task2dnormals)

plotLearnerPrediction(learner = learnerNBay, task = taskSmiley)
plotLearnerPrediction(learner = learnerLDA, task = taskSmiley)
plotLearnerPrediction(learner = learnerQDA, task = taskSmiley)
plotLearnerPrediction(learner = learnerRDA, task = taskSmiley)

plotLearnerPrediction(learner = learnerNBay, task = taskCassini)
plotLearnerPrediction(learner = learnerLDA, task = taskCassini)
plotLearnerPrediction(learner = learnerQDA, task = taskCassini)
plotLearnerPrediction(learner = learnerRDA, task = taskCassini)

```

### b)


